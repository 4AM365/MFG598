{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search database for all CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from m0_1_system_variables import test_data_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a directory of source files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_csv_path_list = []\n",
    "for root, dirs, files in os.walk(test_data_path):\n",
    "        for filename in files:\n",
    "                if filename.lower().endswith('csv'):\n",
    "                    complete_csv_path_list.append(r\"{0}\\{1}\".format(root, filename))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify formatting of source files and import the data to buffer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "\n",
    "data_type = []\n",
    "# Determine the file encoding\n",
    "for file_path in complete_csv_path_list:\n",
    "    with open(file_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "    file_encoding = result['encoding']\n",
    "    data_type.append(file_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ascii', 'ascii', 'ascii', 'ascii', 'ascii', 'ascii', 'ascii', 'ascii', 'ascii', 'ascii', 'ascii', 'ascii', 'ascii', 'ascii', 'ascii', 'ascii', 'ascii', 'ascii', 'ascii', 'Windows-1252', 'Windows-1252', 'ascii', 'ascii', 'ascii', 'ascii', 'ascii', 'ascii', 'ascii', 'ascii', 'ascii']\n"
     ]
    }
   ],
   "source": [
    "print(data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create an empty list to store the dataframes\n",
    "df_list = []\n",
    "vin_list = []\n",
    "# Iterate through each file path in the list\n",
    "for file_path in complete_csv_path_list:\n",
    "    # Read the CSV file into a dataframe\n",
    "    # Difficulty here: some files are ascii and some files are 'Windows-1252' encoding.\n",
    "    with open(file_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "        detected_encoding = result['encoding']\n",
    "\n",
    "    df = pd.read_csv(file_path, delimiter = '\\r\\n', encoding = detected_encoding, engine = 'python', names = ['Complete Record'])\n",
    "    df_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                                       Complete Record\n",
       " 0                                 CDR FILE INFORMATION\n",
       " 1                   User Entered VIN,1FDXE45S29DA10452\n",
       " 2                                   User,M.  Mikhailov\n",
       " 3                            Case Number,01371524-2019\n",
       " 4    EDR Data Imaging Date,\"02/20/2019             ...\n",
       " ..                                                 ...\n",
       " 202                          Contains No Recorded Data\n",
       " 203           LONGITUDINAL CRASH PULSE (SECOND RECORD)\n",
       " 204                          Contains No Recorded Data\n",
       " 205                LATERAL CRASH PULSE (SECOND RECORD)\n",
       " 206                          Contains No Recorded Data\n",
       " \n",
       " [207 rows x 1 columns]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_import_dataframe = pd.DataFrame(columns=['Complete Records'])\n",
    "\n",
    "for object in df_list:\n",
    "    master_import_dataframe = pd.concat([master_import_dataframe, pd.DataFrame({'Complete Records': [object]})], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Complete Records</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Complet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Complete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Complet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Complete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Complet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Complete Records\n",
       "0                                         Complet...\n",
       "1                                        Complete...\n",
       "2                                         Complet...\n",
       "3                                        Complete...\n",
       "4                                         Complet..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_import_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The dataframe is complete, but the CSV is truncated. Something is breaking here.\n",
    "master_import_dataframe.to_csv('master_dataframe.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_import_dataframe[\"Complete Records\"][24].to_csv('test_single_instance.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_import_dataframe_from_file = pd.read_csv('master_dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Complete Records</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Complet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Complete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Complet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Complete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Complet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Complete Records\n",
       "0                                         Complet...\n",
       "1                                        Complete...\n",
       "2                                         Complet...\n",
       "3                                        Complete...\n",
       "4                                         Complet..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_import_dataframe_from_file.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd will export single instances to CSV but does not adequately preserve the entire dataframe. Pickle or Parquet will be more suitable. Because Parquet is interoperable with other programs, it was chosen instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't infer object conversion type: 0                                            Complet...\n1                                           Complete...\n2                                            Complet...\n3                                           Complete...\n4                                            Complet...\n5                                           Complete...\n6                                            Complet...\n7                                            Complet...\n8                                           Complete...\n9                                           Complete...\n10                                           Complet...\n11                                          Complete...\n12                                          Complete...\n13                                           Complet...\n14                                          Complete...\n15                                          Complete...\n16                                           Complet...\n17                                           Complet...\n18                                           Complet...\n19                                           Complet...\n20                                           Complet...\n21                                           Complet...\n22                                          Complete...\n23                                          Complete...\n24                     Complete Record\n0      Claim ...\n25                                          Complete...\n26                                           Complet...\n27                                           Complet...\n28                                           Complet...\n29                                           Complet...\nName: Complete Records, dtype: object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m master_import_dataframe\u001b[39m.\u001b[39;49mto_parquet(\u001b[39m\"\u001b[39;49m\u001b[39mmaster_import_dataframe.parquet\u001b[39;49m\u001b[39m\"\u001b[39;49m, engine \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mfastparquet\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\W Craig\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:2888\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[1;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m   2801\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2802\u001b[0m \u001b[39mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[0;32m   2803\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2884\u001b[0m \u001b[39m>>> content = f.read()\u001b[39;00m\n\u001b[0;32m   2885\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2886\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparquet\u001b[39;00m \u001b[39mimport\u001b[39;00m to_parquet\n\u001b[1;32m-> 2888\u001b[0m \u001b[39mreturn\u001b[39;00m to_parquet(\n\u001b[0;32m   2889\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   2890\u001b[0m     path,\n\u001b[0;32m   2891\u001b[0m     engine,\n\u001b[0;32m   2892\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m   2893\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m   2894\u001b[0m     partition_cols\u001b[39m=\u001b[39;49mpartition_cols,\n\u001b[0;32m   2895\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   2896\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   2897\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\W Craig\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parquet.py:411\u001b[0m, in \u001b[0;36mto_parquet\u001b[1;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[0;32m    407\u001b[0m impl \u001b[39m=\u001b[39m get_engine(engine)\n\u001b[0;32m    409\u001b[0m path_or_buf: FilePath \u001b[39m|\u001b[39m WriteBuffer[\u001b[39mbytes\u001b[39m] \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO() \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m path\n\u001b[1;32m--> 411\u001b[0m impl\u001b[39m.\u001b[39;49mwrite(\n\u001b[0;32m    412\u001b[0m     df,\n\u001b[0;32m    413\u001b[0m     path_or_buf,\n\u001b[0;32m    414\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m    415\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m    416\u001b[0m     partition_cols\u001b[39m=\u001b[39;49mpartition_cols,\n\u001b[0;32m    417\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m    418\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    419\u001b[0m )\n\u001b[0;32m    421\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    422\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, io\u001b[39m.\u001b[39mBytesIO)\n",
      "File \u001b[1;32mc:\\Users\\W Craig\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parquet.py:287\u001b[0m, in \u001b[0;36mFastParquetImpl.write\u001b[1;34m(self, df, path, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mstorage_options passed with file object or non-fsspec file path\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[0;32m    286\u001b[0m \u001b[39mwith\u001b[39;00m catch_warnings(record\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> 287\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi\u001b[39m.\u001b[39;49mwrite(\n\u001b[0;32m    288\u001b[0m         path,\n\u001b[0;32m    289\u001b[0m         df,\n\u001b[0;32m    290\u001b[0m         compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m    291\u001b[0m         write_index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m    292\u001b[0m         partition_on\u001b[39m=\u001b[39;49mpartition_cols,\n\u001b[0;32m    293\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    294\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\W Craig\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fastparquet\\writer.py:1299\u001b[0m, in \u001b[0;36mwrite\u001b[1;34m(filename, data, row_group_offsets, compression, file_scheme, open_with, mkdirs, has_nulls, write_index, partition_on, fixed_text, append, object_encoding, times, custom_metadata, stats)\u001b[0m\n\u001b[0;32m   1296\u001b[0m check_column_names(data\u001b[39m.\u001b[39mcolumns, partition_on, fixed_text,\n\u001b[0;32m   1297\u001b[0m                    object_encoding, has_nulls)\n\u001b[0;32m   1298\u001b[0m ignore \u001b[39m=\u001b[39m partition_on \u001b[39mif\u001b[39;00m file_scheme \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msimple\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m []\n\u001b[1;32m-> 1299\u001b[0m fmd \u001b[39m=\u001b[39m make_metadata(data, has_nulls\u001b[39m=\u001b[39;49mhas_nulls, ignore_columns\u001b[39m=\u001b[39;49mignore,\n\u001b[0;32m   1300\u001b[0m                     fixed_text\u001b[39m=\u001b[39;49mfixed_text,\n\u001b[0;32m   1301\u001b[0m                     object_encoding\u001b[39m=\u001b[39;49mobject_encoding,\n\u001b[0;32m   1302\u001b[0m                     times\u001b[39m=\u001b[39;49mtimes, index_cols\u001b[39m=\u001b[39;49mindex_cols,\n\u001b[0;32m   1303\u001b[0m                     partition_cols\u001b[39m=\u001b[39;49mpartition_on, cols_dtype\u001b[39m=\u001b[39;49mcols_dtype)\n\u001b[0;32m   1304\u001b[0m \u001b[39mif\u001b[39;00m custom_metadata \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1305\u001b[0m     kvm \u001b[39m=\u001b[39m fmd\u001b[39m.\u001b[39mkey_value_metadata \u001b[39mor\u001b[39;00m []\n",
      "File \u001b[1;32mc:\\Users\\W Craig\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fastparquet\\writer.py:902\u001b[0m, in \u001b[0;36mmake_metadata\u001b[1;34m(data, has_nulls, ignore_columns, fixed_text, object_encoding, times, index_cols, partition_cols, cols_dtype)\u001b[0m\n\u001b[0;32m    900\u001b[0m     se\u001b[39m.\u001b[39mname \u001b[39m=\u001b[39m column\n\u001b[0;32m    901\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 902\u001b[0m     se, \u001b[39mtype\u001b[39m \u001b[39m=\u001b[39m find_type(data[column], fixed_text\u001b[39m=\u001b[39;49mfixed,\n\u001b[0;32m    903\u001b[0m                          object_encoding\u001b[39m=\u001b[39;49moencoding, times\u001b[39m=\u001b[39;49mtimes,\n\u001b[0;32m    904\u001b[0m                          is_index\u001b[39m=\u001b[39;49mis_index)\n\u001b[0;32m    905\u001b[0m col_has_nulls \u001b[39m=\u001b[39m has_nulls\n\u001b[0;32m    906\u001b[0m \u001b[39mif\u001b[39;00m has_nulls \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\W Craig\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fastparquet\\writer.py:124\u001b[0m, in \u001b[0;36mfind_type\u001b[1;34m(data, fixed_text, object_encoding, times, is_index)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[39melif\u001b[39;00m dtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mO\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    123\u001b[0m     \u001b[39mif\u001b[39;00m object_encoding \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39minfer\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 124\u001b[0m         object_encoding \u001b[39m=\u001b[39m infer_object_encoding(data)\n\u001b[0;32m    126\u001b[0m     \u001b[39mif\u001b[39;00m object_encoding \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    127\u001b[0m         \u001b[39mtype\u001b[39m, converted_type, width \u001b[39m=\u001b[39m (parquet_thrift\u001b[39m.\u001b[39mType\u001b[39m.\u001b[39mBYTE_ARRAY,\n\u001b[0;32m    128\u001b[0m                                        parquet_thrift\u001b[39m.\u001b[39mConvertedType\u001b[39m.\u001b[39mUTF8,\n\u001b[0;32m    129\u001b[0m                                        \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\W Craig\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\fastparquet\\writer.py:355\u001b[0m, in \u001b[0;36minfer_object_encoding\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    353\u001b[0m     s \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    354\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt infer object conversion type: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m data)\n\u001b[0;32m    356\u001b[0m \u001b[39mif\u001b[39;00m s \u001b[39m>\u001b[39m \u001b[39m10\u001b[39m:\n\u001b[0;32m    357\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Can't infer object conversion type: 0                                            Complet...\n1                                           Complete...\n2                                            Complet...\n3                                           Complete...\n4                                            Complet...\n5                                           Complete...\n6                                            Complet...\n7                                            Complet...\n8                                           Complete...\n9                                           Complete...\n10                                           Complet...\n11                                          Complete...\n12                                          Complete...\n13                                           Complet...\n14                                          Complete...\n15                                          Complete...\n16                                           Complet...\n17                                           Complet...\n18                                           Complet...\n19                                           Complet...\n20                                           Complet...\n21                                           Complet...\n22                                          Complete...\n23                                          Complete...\n24                     Complete Record\n0      Claim ...\n25                                          Complete...\n26                                           Complet...\n27                                           Complet...\n28                                           Complet...\n29                                           Complet...\nName: Complete Records, dtype: object"
     ]
    }
   ],
   "source": [
    "master_import_dataframe.to_parquet(\"master_import_dataframe.parquet\", engine = 'fastparquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet encountered unexplained issues, so pickling will be implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_import_dataframe.to_pickle(\"master_import_dataframe.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
