{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROJECT DESCRIPTION\n",
    "\n",
    "Motor vehicles continuously collect data on driver input parameters, including brake pressure, steering angle, accelerator pedal position, and more. This data is stored in a vehicle’s Airbag Control Module and can be retrieved after the crash. A tool will be created to ingest and prepare this data for preliminary analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOLUTION (Deliverables).\n",
    "\n",
    "•\tAutomate process to open proprietary format in Bosch software and export as CSV.\n",
    "\n",
    "•\tFollow Data Engineering Lifecycle to ingest, anonymize, and store data.\n",
    "\n",
    "•\tHomogenize the dataset so that it can be studied effectively.\n",
    "\n",
    "•\tContinuously discover and ingest data.\n",
    "\n",
    "•\tPerform Experimental Data Analysis with an unfamiliar tool (SpeedML)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASETS: Corporate pre-crash records, "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLAN OF ACTION\n",
    "\n",
    "1) Ingestion\n",
    "\n",
    "    a) Develop a list of filenames and their respective directories, \n",
    "        so that pyAutoGUI can open the files with Bosch CDR\n",
    "\n",
    "    b) Use pyautoGUI to launch each file individually and save a CSV to that same folder.\n",
    "\n",
    "    c) Create code to scrape each CSV into Pandas, either a dedicated dataframe or a master DF so that info can be extracted.\n",
    "\n",
    "    d) Develop a system for re-scanning the database, identifying new files, and then importing only those.\n",
    "    \n",
    "2) Cleaning of data and Exploratory Data Analysis (EDA)\n",
    "\n",
    "    Two things must be accomplished here:\n",
    "    \n",
    "    a) First, we should preserve the granularity of data by importing all crash information.\n",
    "    \n",
    "    b) Next, we should identify elements common to all records so that we can leverage the entire dataset for study.\n",
    "\n",
    "    This is a place where SpeedML may be useful. We don't have to fully clean the data before doing some exploration.\n",
    "\n",
    "3) Model planning\n",
    "\n",
    "    a) Per knowledge, training, and experience, the severity of accidents can be roughly predicted by the peak acceleration values present in the event data recording. Therefore, the model should first bin the records by delta-V (acceleration) with the y-axis representing the quantity of records. An important caveat is that this model will only account for losses of ego vehicles and not the secondary affects e.g. damage to structures, pedestrian strikes, or adverse vehicles.\n",
    "\n",
    "    b) After the results are binned, the equipment numbers present in the record comments can then be correlated with equipment make and model. Certain equipment numbers can be grouped as they map to a single make and model.\n",
    "    \n",
    "    b) Employ pareto principle to identify largest sources of waste. Plot financial impact of incidents, ranked by severity.\n",
    "\n",
    "4) Future Work\n",
    "\n",
    "    a) Package the project as an executable using PyOxidizer so that any person or process can leverage it to scrape data and feed it to the pipeline.\n",
    "\n",
    "    b) Learn how to best manage new vehicles, data, and formats being added to the fleet. This will be accomplished through either complete liberation of the code from the data format (i.e. use RegEx for everything) or a graphical interface for identifying key data and 'training' the code.\n",
    "\n",
    "    c) "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
